---
title: "Barcode data_R1_L001"
author: "Brian Thomas"
date: "2023-05-23"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##Barcode Analysis Pipeline Using Dada2

```{r config}
dir <- "/[your_directory]"
amp_data_dir=file.path(dir, "[your_folder_name]")
list.files(amp_data_dir)
```

## Load libraries and files

```{r libraries}
library(dada2)
library(readr)
library(stringr)
library(dplyr)
library(tibble)
library(magrittr)
library(phyloseq)
library(ggplot2)
library(fs)
library(tidyr)
library(here)
library(tools)
```

```{r}
amp_data_dir %>%
    list.files(pattern="L001_R1_001.fastq.gz",
               full.names=TRUE) %>% sort() -> fnFs

amp_data_dir %>%
    list.files(pattern="L002_R2_001.fastq.gz",
               full.names=TRUE) %>% sort() -> fnRs

print(fnFs)
print(fnRs)
```

Extract sample names, assuming filenames have format: SAMPLENAME.X.fastq, where X is reverse or forwar

```{r}
forward_fastq_suffix = "..............R1_001.fastq.gz"

fnFs %>% 
    basename %>%
    str_remove(forward_fastq_suffix) ->
    sample_namesF

print(sample_namesF)

reverse_fastq_suffix = "..............R2_001.fastq.gz"

fnRs %>% 
    basename %>%
    str_remove(reverse_fastq_suffix) ->
    sample_namesR

print(sample_namesR)
```

Check quality of reads

```{r}

set.seed(100)
#plotQualityProfile(fnFs, aggregate = T) # warning: this will take longer if you ask for more samples
plotQualityProfile(fnFs[1:5], n = 1e+05, aggregate = T) #limited number of bases (100000) and samples (1-5)

```


## Perform filtering and trimming
Assign the filenames for the filtered fastq.gz files.

```{r, filt-names}
filt_path <- file.path(bjt_dir, "filtered") # Place filtered files in filtered/ subdirectory
filtFs <- file.path(filt_path, paste0(sample_namesF, "_F_filt.fastq.gz"))
```

Filter and trim forward reads. Final trim length is 60 nt

```{r, filter R1 only}
filt_out <- filterAndTrim(fnFs, filtFs, trimLeft=20, trimRight = 20, truncLen = 60,
              maxN=0, maxEE=2, truncQ=2, rm.phix=TRUE,
              compress=TRUE, multithread=32) # On Windows set multithread=FALSE


```

The output of `filterAndTrim` has a table summary so we can quickly check the number of reads in and out for each sample like this.
```{r}
filt_out %>%
  data.frame()%>%
  arrange(reads.out) %>%
  head(n = 80)
```

# Learn the Error Rates

The DADA2 algorithm depends on a parametric error model (`err`) and every amplicon dataset has a different set of error rates. The `learnErrors` method learns the error model from the data, by alternating estimation of the error rates and inference of sample composition until they converge on a jointly consistent solution. As in many optimization problems, the algorithm must begin with an initial guess, for which the maximum possible error rates in this data are used (the error rates if only the most abundant sequence is correct and all the rest are errors).


An important note: If some samples ended up with zero reads after `filterAndTrim`, the resulting fastq will not exist because the function won't create empty filtered FASTQs. This can cause errors downstream if you try to feed `learnErrors` the name of a file that does not exist.  Here is a quick way to check that we have all the files we expect:


```{r}
filtFs %>%
  file_exists %>%
   all
```

If there are "missing" filtered FASTQs, those were probably ones with zero reads after filtering.  
Here is how we can subset the vectors of filtered FASTQs to remove any non-existant files.

```{r}
filtFs = filtFs[file_exists(filtFs)]
filtRs = filtRs[file_exists(filtRs)]
```

Learn error rates


```{r}
#errF <- learnErrors(filtFs, multithread=32) #use optional process to decrease burden (used here because too much work above, took 15 min). The second performance-relevant feature to be aware of is that error rates are being learned from a subset of the data. Learning error rates is computationally intensive, as it requires multiple iterations of the core algorithm. As a rule of thumb, a million 100nt reads (or 100M total bases) is more than adequate to learn the error rates.

set.seed(100)
errF <- learnErrors(filtFs, nbases = 1e8, multithread=32, randomize=TRUE)
```

It is always worthwhile, as a reality check if nothing else, to visualize the estimated error rates:

```{r, plot-errors}
plotErrors(errF, nominalQ=TRUE)
```


**Dereplicate the filtered fastq files**
An important note: Remember that we generated our "sample_names" from the original list of FASTQs. Just as above, it is possible that some samples dropped out during filtering, so it is safest to re-generate our vector of sample names to be sure we exclude names of samples that have dropped out.  We can regenerate the sample list based on the list of filtered FASTQs.

```{r}
filtFs %>% 
  basename %>%
  str_replace("_F_filt.fastq.gz","") ->
  sample_names
  

sample_names
```

Dereplicate the filtered FASTQs

```{r, dereplicate}
derepFs <- derepFastq(filtFs, verbose=TRUE)

# Name the derep-class objects by the sample names
names(derepFs) <- sample_names
```



# Sample Inference

We are now ready to apply the core sequence-variant inference algorithm to the dereplicated data. 

**Infer the sequence variants in each sample**

```{r, dada}

dadaFs <- dada(derepFs, err=errF, multithread=32)

```


```{r}
# Optional: infer sequence variants for big data, see: https://benjjneb.github.io/dada2/bigdata.html. The crucial difference between this workflow and the introductory workflow is that the samples are read in and processed in a streaming fashion (within a for-loop) during sample inference, so only one sample is fully loaded into memory at a time. This keeps memory requirements quite low: A Hiseq lane can be processed on 8GB of memory (although more is nice!).

# dadaFs <- vector("list", length(sample_names))
# names(dadaFs) <- sample_names
# for(sam in sample_names) {
#   cat("Processing:", sam, "\n")
#   dadaFs[[sam]] <- dada(derepFs, err=errF, multithread=32)
# }

```


# Construct sequence table

Now construct a sequence table of samples

```{r, seqtab}
# Construct sequence table

seqtab <- makeSequenceTable(dadaFs)
dim(seqtab)

seqtab

# Inspect distribution of sequence lengths
table(nchar(getSequences(seqtab)))

#write to disk
saveRDS(seqtab, "barcode_F_seqtab.rds") # CHANGE ME to where you want sequence table saved

```


# Remove chimeras

The core `dada` method removes substitution and indel errors, but chimeras remain. Fortunately, the accuracy of the sequences after denoising makes identifying chimeras simpler than it is when dealing with fuzzy OTUs: all sequences which can be exactly reconstructed as a bimera (two-parent chimera) from more abundant sequences.

**Remove chimeric sequences**:

```{r, chimeras}
seqtab_nochim <- removeBimeraDenovo(seqtab, method="consensus", multithread=32, verbose=TRUE)
dim(seqtab_nochim)
sum(seqtab_nochim)/sum(seqtab)

#write to disk
saveRDS(seqtab_nochim, "barcode_F_seqnochim.rds") # CHANGE ME to where you want sequence table saved
```
